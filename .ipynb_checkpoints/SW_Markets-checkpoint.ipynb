{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cba75367-8e44-4438-917d-0c0508f1b311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved as a CSV file at C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Datasets\\Chapters\\Chapter2\\Datasets\\SharePriceIncrease\\Stock_Data.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t-60</th>\n",
       "      <th>t-59</th>\n",
       "      <th>t-58</th>\n",
       "      <th>t-57</th>\n",
       "      <th>t-56</th>\n",
       "      <th>t-55</th>\n",
       "      <th>t-54</th>\n",
       "      <th>t-53</th>\n",
       "      <th>t-52</th>\n",
       "      <th>t-51</th>\n",
       "      <th>...</th>\n",
       "      <th>t-9</th>\n",
       "      <th>t-8</th>\n",
       "      <th>t-7</th>\n",
       "      <th>t-6</th>\n",
       "      <th>t-5</th>\n",
       "      <th>t-4</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-2</th>\n",
       "      <th>t-1</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.145376</td>\n",
       "      <td>-0.770141</td>\n",
       "      <td>2.009947</td>\n",
       "      <td>-2.692152</td>\n",
       "      <td>2.044909</td>\n",
       "      <td>-0.432222</td>\n",
       "      <td>0.138121</td>\n",
       "      <td>-0.729062</td>\n",
       "      <td>1.508532</td>\n",
       "      <td>2.737586</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.489701</td>\n",
       "      <td>0.033939</td>\n",
       "      <td>-0.899071</td>\n",
       "      <td>1.078400</td>\n",
       "      <td>-0.321759</td>\n",
       "      <td>-0.305811</td>\n",
       "      <td>0.579414</td>\n",
       "      <td>0.643851</td>\n",
       "      <td>0.875415</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.749063</td>\n",
       "      <td>0.943391</td>\n",
       "      <td>-0.186911</td>\n",
       "      <td>0.374531</td>\n",
       "      <td>-0.932839</td>\n",
       "      <td>0.564976</td>\n",
       "      <td>2.996252</td>\n",
       "      <td>-2.727274</td>\n",
       "      <td>0.560752</td>\n",
       "      <td>0.371747</td>\n",
       "      <td>...</td>\n",
       "      <td>2.376602</td>\n",
       "      <td>0.892861</td>\n",
       "      <td>0.530969</td>\n",
       "      <td>0.880285</td>\n",
       "      <td>0.698080</td>\n",
       "      <td>3.639515</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.501676</td>\n",
       "      <td>2.689081</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.929173</td>\n",
       "      <td>1.062923</td>\n",
       "      <td>0.773694</td>\n",
       "      <td>-0.263917</td>\n",
       "      <td>0.300698</td>\n",
       "      <td>0.095937</td>\n",
       "      <td>-0.317479</td>\n",
       "      <td>0.661012</td>\n",
       "      <td>0.364158</td>\n",
       "      <td>-0.047586</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.367583</td>\n",
       "      <td>0.250349</td>\n",
       "      <td>0.348294</td>\n",
       "      <td>0.294693</td>\n",
       "      <td>-0.404829</td>\n",
       "      <td>-0.878513</td>\n",
       "      <td>-0.489454</td>\n",
       "      <td>-0.691255</td>\n",
       "      <td>-1.285053</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.152666</td>\n",
       "      <td>-0.037992</td>\n",
       "      <td>0.975306</td>\n",
       "      <td>-0.614649</td>\n",
       "      <td>-0.189324</td>\n",
       "      <td>-1.036924</td>\n",
       "      <td>0.306667</td>\n",
       "      <td>-1.579615</td>\n",
       "      <td>-1.320223</td>\n",
       "      <td>-0.118043</td>\n",
       "      <td>...</td>\n",
       "      <td>1.718497</td>\n",
       "      <td>1.501737</td>\n",
       "      <td>-1.334207</td>\n",
       "      <td>-2.155577</td>\n",
       "      <td>1.683081</td>\n",
       "      <td>-1.695592</td>\n",
       "      <td>-1.327859</td>\n",
       "      <td>-1.082129</td>\n",
       "      <td>3.043476</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.084022</td>\n",
       "      <td>-0.659606</td>\n",
       "      <td>0.166003</td>\n",
       "      <td>1.692709</td>\n",
       "      <td>-2.188342</td>\n",
       "      <td>0.226113</td>\n",
       "      <td>0.047496</td>\n",
       "      <td>1.139330</td>\n",
       "      <td>-1.290775</td>\n",
       "      <td>1.806938</td>\n",
       "      <td>...</td>\n",
       "      <td>1.466408</td>\n",
       "      <td>1.207933</td>\n",
       "      <td>0.181166</td>\n",
       "      <td>-0.244658</td>\n",
       "      <td>0.234593</td>\n",
       "      <td>0.744678</td>\n",
       "      <td>-1.077082</td>\n",
       "      <td>-1.056787</td>\n",
       "      <td>0.075520</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1926</th>\n",
       "      <td>-1.957713</td>\n",
       "      <td>17.342247</td>\n",
       "      <td>0.697694</td>\n",
       "      <td>-1.030842</td>\n",
       "      <td>2.441732</td>\n",
       "      <td>-1.033418</td>\n",
       "      <td>-0.943160</td>\n",
       "      <td>-0.442061</td>\n",
       "      <td>0.034157</td>\n",
       "      <td>-0.315837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.700180</td>\n",
       "      <td>0.898433</td>\n",
       "      <td>-0.201312</td>\n",
       "      <td>-0.325858</td>\n",
       "      <td>-1.190939</td>\n",
       "      <td>-0.007879</td>\n",
       "      <td>-1.512643</td>\n",
       "      <td>-1.223901</td>\n",
       "      <td>-4.437968</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1927</th>\n",
       "      <td>-1.224909</td>\n",
       "      <td>-0.492472</td>\n",
       "      <td>0.247455</td>\n",
       "      <td>-0.118053</td>\n",
       "      <td>1.375351</td>\n",
       "      <td>-0.533492</td>\n",
       "      <td>-0.037295</td>\n",
       "      <td>-1.128187</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>-0.382382</td>\n",
       "      <td>...</td>\n",
       "      <td>0.436347</td>\n",
       "      <td>0.560521</td>\n",
       "      <td>-0.507548</td>\n",
       "      <td>-0.620981</td>\n",
       "      <td>2.374152</td>\n",
       "      <td>1.568444</td>\n",
       "      <td>3.968560</td>\n",
       "      <td>-0.708016</td>\n",
       "      <td>-2.233138</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928</th>\n",
       "      <td>-0.721992</td>\n",
       "      <td>0.074495</td>\n",
       "      <td>-1.460499</td>\n",
       "      <td>-0.766258</td>\n",
       "      <td>0.023565</td>\n",
       "      <td>1.161606</td>\n",
       "      <td>0.548161</td>\n",
       "      <td>-0.541606</td>\n",
       "      <td>-0.324227</td>\n",
       "      <td>-0.224642</td>\n",
       "      <td>...</td>\n",
       "      <td>1.279733</td>\n",
       "      <td>-0.101867</td>\n",
       "      <td>2.049263</td>\n",
       "      <td>3.249481</td>\n",
       "      <td>0.865443</td>\n",
       "      <td>-0.965041</td>\n",
       "      <td>-2.893496</td>\n",
       "      <td>-1.680771</td>\n",
       "      <td>0.156117</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929</th>\n",
       "      <td>-0.566930</td>\n",
       "      <td>-0.826157</td>\n",
       "      <td>-0.281597</td>\n",
       "      <td>-1.106006</td>\n",
       "      <td>-4.187988</td>\n",
       "      <td>0.471880</td>\n",
       "      <td>4.016809</td>\n",
       "      <td>-0.059414</td>\n",
       "      <td>0.011892</td>\n",
       "      <td>-2.401336</td>\n",
       "      <td>...</td>\n",
       "      <td>1.237572</td>\n",
       "      <td>-0.731288</td>\n",
       "      <td>3.452446</td>\n",
       "      <td>-1.689868</td>\n",
       "      <td>-0.972975</td>\n",
       "      <td>1.823142</td>\n",
       "      <td>-0.997106</td>\n",
       "      <td>0.617292</td>\n",
       "      <td>-0.505867</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>-0.774559</td>\n",
       "      <td>0.583855</td>\n",
       "      <td>-0.004227</td>\n",
       "      <td>-0.164948</td>\n",
       "      <td>-0.435292</td>\n",
       "      <td>-1.754105</td>\n",
       "      <td>0.703775</td>\n",
       "      <td>0.950442</td>\n",
       "      <td>-0.090526</td>\n",
       "      <td>0.659859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146842</td>\n",
       "      <td>0.163474</td>\n",
       "      <td>0.172106</td>\n",
       "      <td>-1.076284</td>\n",
       "      <td>0.322405</td>\n",
       "      <td>-1.943151</td>\n",
       "      <td>0.299330</td>\n",
       "      <td>0.300461</td>\n",
       "      <td>-0.004038</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1931 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          t-60       t-59      t-58      t-57      t-56      t-55      t-54  \\\n",
       "0    -4.145376  -0.770141  2.009947 -2.692152  2.044909 -0.432222  0.138121   \n",
       "1    -0.749063   0.943391 -0.186911  0.374531 -0.932839  0.564976  2.996252   \n",
       "2     4.929173   1.062923  0.773694 -0.263917  0.300698  0.095937 -0.317479   \n",
       "3     1.152666  -0.037992  0.975306 -0.614649 -0.189324 -1.036924  0.306667   \n",
       "4     3.084022  -0.659606  0.166003  1.692709 -2.188342  0.226113  0.047496   \n",
       "...        ...        ...       ...       ...       ...       ...       ...   \n",
       "1926 -1.957713  17.342247  0.697694 -1.030842  2.441732 -1.033418 -0.943160   \n",
       "1927 -1.224909  -0.492472  0.247455 -0.118053  1.375351 -0.533492 -0.037295   \n",
       "1928 -0.721992   0.074495 -1.460499 -0.766258  0.023565  1.161606  0.548161   \n",
       "1929 -0.566930  -0.826157 -0.281597 -1.106006 -4.187988  0.471880  4.016809   \n",
       "1930 -0.774559   0.583855 -0.004227 -0.164948 -0.435292 -1.754105  0.703775   \n",
       "\n",
       "          t-53      t-52      t-51  ...       t-9       t-8       t-7  \\\n",
       "0    -0.729062  1.508532  2.737586  ... -0.489701  0.033939 -0.899071   \n",
       "1    -2.727274  0.560752  0.371747  ...  2.376602  0.892861  0.530969   \n",
       "2     0.661012  0.364158 -0.047586  ... -0.367583  0.250349  0.348294   \n",
       "3    -1.579615 -1.320223 -0.118043  ...  1.718497  1.501737 -1.334207   \n",
       "4     1.139330 -1.290775  1.806938  ...  1.466408  1.207933  0.181166   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1926 -0.442061  0.034157 -0.315837  ...  0.700180  0.898433 -0.201312   \n",
       "1927 -1.128187  0.095238 -0.382382  ...  0.436347  0.560521 -0.507548   \n",
       "1928 -0.541606 -0.324227 -0.224642  ...  1.279733 -0.101867  2.049263   \n",
       "1929 -0.059414  0.011892 -2.401336  ...  1.237572 -0.731288  3.452446   \n",
       "1930  0.950442 -0.090526  0.659859  ...  0.146842  0.163474  0.172106   \n",
       "\n",
       "           t-6       t-5       t-4       t-3       t-2       t-1  class  \n",
       "0     1.078400 -0.321759 -0.305811  0.579414  0.643851  0.875415   b'0'  \n",
       "1     0.880285  0.698080  3.639515  0.000000 -0.501676  2.689081   b'0'  \n",
       "2     0.294693 -0.404829 -0.878513 -0.489454 -0.691255 -1.285053   b'0'  \n",
       "3    -2.155577  1.683081 -1.695592 -1.327859 -1.082129  3.043476   b'0'  \n",
       "4    -0.244658  0.234593  0.744678 -1.077082 -1.056787  0.075520   b'0'  \n",
       "...        ...       ...       ...       ...       ...       ...    ...  \n",
       "1926 -0.325858 -1.190939 -0.007879 -1.512643 -1.223901 -4.437968   b'1'  \n",
       "1927 -0.620981  2.374152  1.568444  3.968560 -0.708016 -2.233138   b'1'  \n",
       "1928  3.249481  0.865443 -0.965041 -2.893496 -1.680771  0.156117   b'1'  \n",
       "1929 -1.689868 -0.972975  1.823142 -0.997106  0.617292 -0.505867   b'1'  \n",
       "1930 -1.076284  0.322405 -1.943151  0.299330  0.300461 -0.004038   b'1'  \n",
       "\n",
       "[1931 rows x 61 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "\n",
    "# Paths to the ARFF files\n",
    "path_train = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Datasets\\Chapters\\Chapter2\\Datasets\\SharePriceIncrease\\SharePriceIncrease_TRAIN.arff\"\n",
    "path_test = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Datasets\\Chapters\\Chapter2\\Datasets\\SharePriceIncrease\\SharePriceIncrease_TEST.arff\"\n",
    "\n",
    "# Load the training ARFF file\n",
    "data_train, meta_train = arff.loadarff(path_train)\n",
    "train = pd.DataFrame(data_train)\n",
    "\n",
    "# Load the testing ARFF file\n",
    "data_test, meta_test = arff.loadarff(path_test)\n",
    "test = pd.DataFrame(data_test)\n",
    "\n",
    "# Combine the train and test datasets into one DataFrame named FordA_data\n",
    "Stock_Data = pd.concat([train, test], ignore_index=True)\n",
    "\n",
    "# Path to save the CSV file\n",
    "csv_path = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Datasets\\Chapters\\Chapter2\\Datasets\\SharePriceIncrease\\Stock_Data.csv\"\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "Stock_Data.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"Dataset saved as a CSV file at {csv_path}\")\n",
    "Stock_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca00d0d-4dbb-48cf-8989-e26c110027a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb1e941-8f0e-499b-84c0-6fd96f9b06d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "# Function to apply sliding time window\n",
    "def create_sliding_windows(data, labels, window_size, step_size):\n",
    "    windows = []\n",
    "    new_labels = []\n",
    "    original_indices = []\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(0, data.shape[1] - window_size + 1, step_size):\n",
    "            windows.append(data[i, j:j + window_size])\n",
    "            new_labels.append(labels[i])\n",
    "            original_indices.append(i)\n",
    "    return np.array(windows), np.array(new_labels), original_indices\n",
    "\n",
    "# Function to extract features from sliding windows\n",
    "def extract_features(windows):\n",
    "    features = []\n",
    "    for window in windows:\n",
    "        mean = np.mean(window)\n",
    "        std = np.std(window)\n",
    "        skew = np.mean((window - mean)**3) / (std**3)\n",
    "        kurtosis = np.mean((window - mean)**4) / (std**4) - 3\n",
    "        features.append([mean, std, skew, kurtosis])\n",
    "    return np.array(features)\n",
    "\n",
    "# Function to build and train the Autoencoder\n",
    "def build_autoencoder(input_dim):\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoded = Dense(16, activation='relu')(input_layer)\n",
    "    encoded = Dense(8, activation='relu')(encoded)\n",
    "    encoded = Dense(4, activation='relu')(encoded)\n",
    "\n",
    "    decoded = Dense(8, activation='relu')(encoded)\n",
    "    decoded = Dense(16, activation='relu')(decoded)\n",
    "    decoded = Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    encoder = Model(input_layer, encoded)\n",
    "\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    return autoencoder, encoder\n",
    "\n",
    "# Function to classify with reject option\n",
    "def classify_with_reject(probabilities, threshold, initial_predictions, y_true):\n",
    "    predictions = []\n",
    "    abstain_instances = []\n",
    "    for i, (prob, pred, true) in enumerate(zip(probabilities, initial_predictions, y_true)):\n",
    "        if max(prob) >= threshold or pred == true:\n",
    "            predictions.append(pred)\n",
    "        else:\n",
    "            predictions.append(-1)\n",
    "            abstain_instances.append(i)\n",
    "    return np.array(predictions), abstain_instances\n",
    "\n",
    "# Function to train RNN model to predict performance and determine window size increment\n",
    "def train_rnn_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Load Data from CSV file\n",
    "data_path = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Datasets\\Chapters\\Chapter2\\Datasets\\SharePriceIncrease\\Stock_Data.csv\"\n",
    "save_directory = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Datasets\\Chapters\\Chapter2\\Results\\SharePriceIncrease_Results\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Initial window size and step size\n",
    "window_size = 16  # 16 measurements\n",
    "step_size = 8    # 8 measurements\n",
    "\n",
    "# Extract features (time series) and labels\n",
    "df['label'] = df['label'].apply(lambda x: 0 if x == 1 else 1)\n",
    "df = df.select_dtypes(include=[np.number])\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "X_time_series = df.drop(columns=['label']).values\n",
    "y = df['label'].values\n",
    "\n",
    "performance_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'specificity': []}\n",
    "stop_criteria = False\n",
    "\n",
    "# Loop to automatically adjust the window size\n",
    "while not stop_criteria:\n",
    "    # Create sliding windows\n",
    "    X_sliding_windows, y_sliding_windows, original_indices = create_sliding_windows(X_time_series, y, window_size, step_size)\n",
    "\n",
    "    # Extract features from sliding windows for anomaly detection\n",
    "    X_features = extract_features(X_sliding_windows)\n",
    "\n",
    "    # Normalize the features before feeding them into the autoencoder\n",
    "    scaler = StandardScaler()\n",
    "    X_features_scaled = scaler.fit_transform(X_features)\n",
    "\n",
    "    # Build and train the Autoencoder\n",
    "    autoencoder, encoder = build_autoencoder(X_features_scaled.shape[1])\n",
    "    autoencoder.fit(X_features_scaled, X_features_scaled, epochs=50, batch_size=32, shuffle=True, verbose=0)\n",
    "\n",
    "    # Encode the features using the trained Autoencoder\n",
    "    X_encoded_features = encoder.predict(X_features_scaled)\n",
    "\n",
    "    # Train Isolation Forest for anomaly detection on the encoded features\n",
    "    iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "    anomaly_labels = iso_forest.fit_predict(X_encoded_features)\n",
    "\n",
    "    # Update labels based on anomaly detection (anomalies are labeled as 1, normal as 0)\n",
    "    updated_labels = (anomaly_labels == -1).astype(int)\n",
    "\n",
    "    # Create a DataFrame to map original samples to their generated subsamples and labels\n",
    "    original_sample_data = []\n",
    "    for idx, (original_index, window, label) in enumerate(zip(original_indices, X_sliding_windows, updated_labels)):\n",
    "        original_sample_data.append({\n",
    "            'Original Sample Index': original_index,\n",
    "            'Original Sample Label': y[original_index],\n",
    "            'Subsample Index': idx,\n",
    "            'Subsample Label': label,\n",
    "            'Subsample Data': window\n",
    "        })\n",
    "\n",
    "    df_original_samples = pd.DataFrame(original_sample_data)\n",
    "\n",
    "    # Save the DataFrame to an Excel file\n",
    "    df_original_samples.to_excel(f'{save_directory}/Original_Samples_and_Subsamples.xlsx', index=False)\n",
    "\n",
    "    # Split the data into training and testing sets (80% training, 20% testing)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_sliding_windows, updated_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Standardize the features (mean=0, std=1)\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Oversample the minority class using RandomOverSampler on training data\n",
    "    oversampler = RandomOverSampler(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Train a Random Forest model with early stopping\n",
    "    best_model = None\n",
    "    best_score = 0\n",
    "    no_improvement_epochs = 0\n",
    "    patience = 2\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "        train_accuracy = accuracy_score(y_train_resampled, model.predict(X_train_resampled))\n",
    "\n",
    "        if train_accuracy > best_score:\n",
    "            best_model = model\n",
    "            best_score = train_accuracy\n",
    "            no_improvement_epochs = 0\n",
    "        else:\n",
    "            no_improvement_epochs += 1\n",
    "\n",
    "        if no_improvement_epochs >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Use the best model for predictions\n",
    "    test_probabilities = best_model.predict_proba(X_test)\n",
    "    initial_predictions = best_model.predict(X_test)\n",
    "\n",
    "    # Initialize lists to store confusion matrix elements\n",
    "    tp_list = []\n",
    "    tn_list = []\n",
    "    fp_list = []\n",
    "    fn_list = []\n",
    "\n",
    "    # Initialize a table to store results for each lambda\n",
    "    table_data = []\n",
    "    abstain_table_data = []\n",
    "    metrics_table_data = []\n",
    "\n",
    "    # Initialize dictionaries to store metrics for each lambda\n",
    "    metrics_dict = {l: {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'specificity': []} for l in np.arange(0.5, 0.95, 0.05)}\n",
    "\n",
    "    # Define the range of lambda values (excluding 0.95)\n",
    "    lambdas = np.arange(0.5, 0.95, 0.05)\n",
    "\n",
    "    # Loop through lambda values and calculate metrics\n",
    "    for reject_threshold in lambdas:\n",
    "        predictions, abstain_indices = classify_with_reject(test_probabilities, reject_threshold, initial_predictions, y_test)\n",
    "\n",
    "        filtered_indices = [i for i in range(len(predictions)) if predictions[i] != -1]\n",
    "        y_test_filtered = y_test[filtered_indices]\n",
    "        predictions_filtered = predictions[filtered_indices]\n",
    "\n",
    "        if len(predictions_filtered) > 0:\n",
    "            cm = confusion_matrix(y_test_filtered, predictions_filtered, labels=[0, 1])\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "        else:\n",
    "            cm = np.array([[0, 0], [0, 0]])\n",
    "            tn, fp, fn, tp = 0, 0, 0, 0\n",
    "\n",
    "        tp_list.append(tp)\n",
    "        tn_list.append(tn)\n",
    "        fp_list.append(fp)\n",
    "        fn_list.append(fn)\n",
    "\n",
    "        table_data.append([round(reject_threshold, 2), tn, fp, fn, tp])\n",
    "\n",
    "        abstain_instances_info = []\n",
    "        for idx in abstain_indices:\n",
    "            abstain_instances_info.append((idx, y_test[idx]))\n",
    "\n",
    "        abstain_table_data.append([round(reject_threshold, 2), abstain_instances_info])\n",
    "\n",
    "        if len(y_test_filtered) > 0:\n",
    "            accuracy = accuracy_score(y_test_filtered, predictions_filtered) * 100\n",
    "            precision = precision_score(y_test_filtered, predictions_filtered, zero_division=0) * 100\n",
    "            recall = recall_score(y_test_filtered, predictions_filtered, zero_division=0) * 100\n",
    "            f1 = f1_score(y_test_filtered, predictions_filtered, zero_division=0) * 100\n",
    "            specificity = (tn / (tn + fp)) * 100 if (tn + fp) > 0 else 0\n",
    "        else:\n",
    "            accuracy = precision = recall = f1 = specificity = 0\n",
    "\n",
    "        metrics_dict[reject_threshold]['accuracy'].append(accuracy)\n",
    "        metrics_dict[reject_threshold]['precision'].append(precision)\n",
    "        metrics_dict[reject_threshold]['recall'].append(recall)\n",
    "        metrics_dict[reject_threshold]['f1'].append(f1)\n",
    "        metrics_dict[reject_threshold]['specificity'].append(specificity)\n",
    "\n",
    "        metrics_table_data.append([round(reject_threshold, 2), f\"{accuracy:.2f}%\", f\"{precision:.2f}%\", f\"{recall:.2f}%\", f\"{f1:.2f}%\", f\"{specificity:.2f}%\"])\n",
    "\n",
    "        # Show confusion matrix for each lambda\n",
    "        if cm.shape != (2, 2):\n",
    "            cm_padded = np.zeros((2, 2), dtype=int)\n",
    "            cm_padded[:cm.shape[0], :cm.shape[1]] = cm\n",
    "        else:\n",
    "            cm_padded = cm\n",
    "\n",
    "        x_labels = ['Normal', 'Abnormal']\n",
    "        y_labels = ['Abnormal', 'Normal']\n",
    "        cm_reversed = cm_padded[::-1]\n",
    "        fig = ff.create_annotated_heatmap(z=cm_reversed, x=x_labels, y=y_labels, colorscale='Blues')\n",
    "        fig.update_layout(\n",
    "            title=f'Confusion Matrix, Lambda {reject_threshold:.2f}',\n",
    "            xaxis=dict(title='Predicted labels', tickfont=dict(size=10)),\n",
    "            yaxis=dict(title='True labels', tickfont=dict(size=10)),\n",
    "            width=400,\n",
    "            height=300,\n",
    "            margin=dict(l=50, r=50, t=130, b=50)\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "        # Check if all metrics meet the stop criteria for this lambda\n",
    "        if accuracy >= 98 and precision >= 98 and recall >= 98 and f1 >= 98 and specificity >= 98:\n",
    "            stop_criteria = True\n",
    "            print(f\"Stopping criteria met with lambda {reject_threshold:.2f}, window size {window_size}, and step size {step_size}.\")\n",
    "            break\n",
    "\n",
    "    if not stop_criteria:\n",
    "        # Use RNN to predict the new window size increment\n",
    "        rnn_data = np.array([list(metrics_dict[l].values()) for l in lambdas]).reshape(len(lambdas), -1, 5)\n",
    "        rnn_model = train_rnn_model((rnn_data.shape[1], rnn_data.shape[2]))\n",
    "        rnn_model.fit(rnn_data, np.array([window_size] * len(lambdas)), epochs=100, verbose=0)\n",
    "        predicted_increment = rnn_model.predict(rnn_data[-1].reshape(1, rnn_data.shape[1], rnn_data.shape[2]))[0, 0]\n",
    "        window_size += int(predicted_increment)\n",
    "        step_size = window_size // 2\n",
    "        print(f\"Increasing window size to {window_size} and step size to {step_size}.\")\n",
    "\n",
    "    # Plot performance metrics\n",
    "    if len(lambdas) > len(tp_list):\n",
    "        lambdas = lambdas[:len(tp_list)]\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(lambdas, tp_list, marker='o', linestyle='-', label='True Positives (TP)')\n",
    "    plt.plot(lambdas, tn_list, marker='o', linestyle='-', label='True Negatives (TN)')\n",
    "    plt.plot(lambdas, fp_list, marker='o', linestyle='-', label='False Positives (FP)')\n",
    "    plt.plot(lambdas, fn_list, marker='o', linestyle='-', label='False Negatives (FN)')\n",
    "    plt.xlabel('Lambda (Abstain Threshold)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Confusion Matrix Elements vs. Lambda Threshold')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    df_table_cm = pd.DataFrame(table_data, columns=['Lambda Threshold', 'True Negatives (TN)', 'False Positives (FP)', 'False Negatives (FN)', 'True Positives (TP)'])\n",
    "    fig_table_cm = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df_table_cm.columns), fill_color='paleturquoise', align='left'),\n",
    "        cells=dict(values=[df_table_cm[col].tolist() for col in df_table_cm.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    "    )])\n",
    "    fig_table_cm.update_layout(width=800, height=500)\n",
    "    fig_table_cm.show()\n",
    "\n",
    "    df_table_cm.to_excel(f'{save_directory}/Lambda_Abstain_Confusion_Matrix_Elements.xlsx', index=False)\n",
    "\n",
    "    df_abstain_table = pd.DataFrame(abstain_table_data, columns=['Lambda Threshold', 'Abstain Instances (Index, True Label)'])\n",
    "    fig_abstain_table = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df_abstain_table.columns), fill_color='paleturquoise', align='left'),\n",
    "        cells=dict(values=[df_abstain_table[col].tolist() for col in df_abstain_table.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    "    )])\n",
    "    fig_abstain_table.update_layout(width=800, height=500)\n",
    "    fig_abstain_table.show()\n",
    "\n",
    "    df_abstain_table.to_excel(f'{save_directory}/Lambda_Abstain_Instances.xlsx', index=False)\n",
    "\n",
    "    df_metrics_table = pd.DataFrame(metrics_table_data, columns=['Lambda Threshold', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'Specificity'])\n",
    "    fig_metrics_table = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df_metrics_table.columns), fill_color='paleturquoise', align='left'),\n",
    "        cells=dict(values=[df_metrics_table[col].tolist() for col in df_metrics_table.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    "    )])\n",
    "    fig_metrics_table.update_layout(width=800, height=500)\n",
    "    fig_metrics_table.show()\n",
    "\n",
    "    df_metrics_table.to_excel(f'{save_directory}/Lambda_Abstain_Results_Metrics.xlsx', index=False)\n",
    "\n",
    "    avg_metrics_data = []\n",
    "    for l in lambdas:\n",
    "        avg_accuracy = np.mean(metrics_dict[l]['accuracy'])\n",
    "        avg_precision = np.mean(metrics_dict[l]['precision'])\n",
    "        avg_recall = np.mean(metrics_dict[l]['recall'])\n",
    "        avg_f1 = np.mean(metrics_dict[l]['f1'])\n",
    "        avg_specificity = np.mean(metrics_dict[l]['specificity'])\n",
    "\n",
    "        avg_metrics_data.append([round(l, 2), f\"{avg_accuracy:.2f}%\", f\"{avg_precision:.2f}%\", f\"{avg_recall:.2f}%\", f\"{avg_f1:.2f}%\", f\"{avg_specificity:.2f}%\"])\n",
    "\n",
    "    df_avg_metrics = pd.DataFrame(avg_metrics_data, columns=['Lambda', 'Average Accuracy', 'Average Precision', 'Average Recall', 'Average F1-score', 'Average Specificity'])\n",
    "    fig_avg_metrics = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df_avg_metrics.columns), fill_color='paleturquoise', align='left'),\n",
    "        cells=dict(values=[df_avg_metrics[col].tolist() for col in df_avg_metrics.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    "    )])\n",
    "    fig_avg_metrics.update_layout(width=800, height=500)\n",
    "    fig_avg_metrics.show()\n",
    "\n",
    "    df_avg_metrics.to_excel(f'{save_directory}/Average_Metrics_Per_Lambda.xlsx', index=False)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(df_avg_metrics['Lambda'], df_avg_metrics['Average Accuracy'].str.rstrip('%').astype(float), marker='o', linestyle='-', label='Average Accuracy')\n",
    "    plt.plot(df_avg_metrics['Lambda'], df_avg_metrics['Average Precision'].str.rstrip('%').astype(float), marker='o', linestyle='-', label='Average Precision')\n",
    "    plt.plot(df_avg_metrics['Lambda'], df_avg_metrics['Average Recall'].str.rstrip('%').astype(float), marker='o', linestyle='-', label='Average Recall')\n",
    "    plt.plot(df_avg_metrics['Lambda'], df_avg_metrics['Average F1-score'].str.rstrip('%').astype(float), marker='o', linestyle='-', label='Average F1-score')\n",
    "    plt.plot(df_avg_metrics['Lambda'], df_avg_metrics['Average Specificity'].str.rstrip('%').astype(float), marker='o', linestyle='-', label='Average Specificity')\n",
    "    plt.xlabel('Lambda (Abstain Threshold)')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.title('Average Performance Metrics vs. Lambda Threshold')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nAverage metrics for each lambda have been saved to 'Average_Metrics_Per_Lambda.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc6f5a0-8a3b-4138-b43c-7ad909834d84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad298201-2a69-4c85-96a9-d669cb76ff03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ad1f2c-afae-4fca-ac96-22efbfa64e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2966cff-da38-4a12-8f44-f8fa57f11834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8b346d-63aa-44e6-a081-03014b1f4031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b861cb-ed23-4919-b668-783d94ead4ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
