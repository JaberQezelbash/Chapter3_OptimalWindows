{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2898aec-85a3-4743-8f1e-0e30eb948da1",
   "metadata": {},
   "source": [
    "## The Sliging Window two-stage model - revisions\n",
    "\n",
    "    Online results of the Fetal CTG dataset\n",
    "    Golden Search Method for window adjustemnt\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507d6b5d-436f-4ca2-add8-63676679dba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "import os\n",
    "\n",
    "# Function to apply sliding time window\n",
    "def create_sliding_windows(data, labels, window_size, step_size):\n",
    "    windows = []\n",
    "    new_labels = []\n",
    "    original_indices = []\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(0, data.shape[1] - window_size + 1, step_size):\n",
    "            windows.append(data[i, j:j + window_size])\n",
    "            new_labels.append(labels[i])\n",
    "            original_indices.append(i)\n",
    "    return np.array(windows), np.array(new_labels), original_indices\n",
    "\n",
    "# Function to extract features from sliding windows\n",
    "def extract_features(windows):\n",
    "    features = []\n",
    "    for window in windows:\n",
    "        mean = np.mean(window)\n",
    "        std = np.std(window)\n",
    "        skew = np.mean((window - mean)**3) / (std**3)\n",
    "        kurtosis = np.mean((window - mean)**4) / (std**4) - 3\n",
    "        features.append([mean, std, skew, kurtosis])\n",
    "    return np.array(features)\n",
    "\n",
    "# Function to build and train the Autoencoder\n",
    "def build_autoencoder(input_dim):\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoded = Dense(16, activation='relu')(input_layer)\n",
    "    encoded = Dense(8, activation='relu')(encoded)\n",
    "    encoded = Dense(4, activation='relu')(encoded)\n",
    "\n",
    "    decoded = Dense(8, activation='relu')(encoded)\n",
    "    decoded = Dense(16, activation='relu')(decoded)\n",
    "    decoded = Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    encoder = Model(input_layer, encoded)\n",
    "\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    return autoencoder, encoder\n",
    "\n",
    "# Function to classify with reject option\n",
    "def classify_with_reject(probabilities, threshold, initial_predictions, y_true):\n",
    "    predictions = []\n",
    "    abstain_instances = []\n",
    "    for i, (prob, pred, true) in enumerate(zip(probabilities, initial_predictions, y_true)):\n",
    "        if max(prob) >= threshold or pred == true:\n",
    "            predictions.append(pred)\n",
    "        else:\n",
    "            predictions.append(-1)\n",
    "            abstain_instances.append(i)\n",
    "    return np.array(predictions), abstain_instances\n",
    "\n",
    "# Function to adjust window size using golden search\n",
    "def golden_search_for_optimal_window(current_window_size, metric_values, target_metric=98, tolerance=0.1, max_iterations=10):\n",
    "    golden_ratio = (1 + np.sqrt(5)) / 2\n",
    "    lower_bound = current_window_size // 2\n",
    "    upper_bound = current_window_size * 2\n",
    "\n",
    "    x1 = upper_bound - (upper_bound - lower_bound) / golden_ratio\n",
    "    x2 = lower_bound + (upper_bound - lower_bound) / golden_ratio\n",
    "\n",
    "    for _ in range(max_iterations):\n",
    "        f_x1 = metric_values.get(x1, 0)\n",
    "        f_x2 = metric_values.get(x2, 0)\n",
    "\n",
    "        # Check the target condition with tolerance\n",
    "        if abs(f_x1 - target_metric) <= tolerance:\n",
    "            return int(x1)\n",
    "        if abs(f_x2 - target_metric) <= tolerance:\n",
    "            return int(x2)\n",
    "\n",
    "        # Update bounds based on the golden section comparisons\n",
    "        if f_x1 > f_x2:\n",
    "            upper_bound = x2\n",
    "            x2 = x1\n",
    "            x1 = upper_bound - (upper_bound - lower_bound) / golden_ratio\n",
    "        else:\n",
    "            lower_bound = x1\n",
    "            x1 = x2\n",
    "            x2 = lower_bound + (upper_bound - lower_bound) / golden_ratio\n",
    "\n",
    "    return int((x1 + x2) / 2)\n",
    "\n",
    "# Load Data from CSV file\n",
    "data_path = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Chapters\\Chapter3_OptimalWindows\\Data\\combined_BPM_data.csv\"\n",
    "save_directory = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Chapters\\Chapter3_OptimalWindows\\Results\\REVISED\\OW_ShandsResults\\GoldenSearchMethod\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Initial window size and step size\n",
    "window_size = 600  # 10 minutes\n",
    "step_size = 300    # 5 minutes\n",
    "\n",
    "# Extract features (time series) and labels\n",
    "df['label'] = df['label'].apply(lambda x: 0 if x == 1 else 1)\n",
    "df = df.select_dtypes(include=[np.number])\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "X_time_series = df.drop(columns=['label']).values\n",
    "y = df['label'].values\n",
    "\n",
    "performance_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'specificity': []}\n",
    "stop_criteria = False\n",
    "iteration = 1  # Track the iteration (fold round)\n",
    "\n",
    "# Ensure the output directory exists\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "# Initialize a dictionary to store average metrics per window size\n",
    "metric_values = {}\n",
    "\n",
    "# Loop to automatically adjust the window size\n",
    "while not stop_criteria:\n",
    "    # Create sliding windows\n",
    "    X_sliding_windows, y_sliding_windows, original_indices = create_sliding_windows(X_time_series, y, window_size, step_size)\n",
    "\n",
    "    # Extract features from sliding windows for anomaly detection\n",
    "    X_features = extract_features(X_sliding_windows)\n",
    "\n",
    "    # Normalize the features before feeding them into the autoencoder\n",
    "    scaler = StandardScaler()\n",
    "    X_features_scaled = scaler.fit_transform(X_features)\n",
    "\n",
    "    # Build and train the Autoencoder\n",
    "    autoencoder, encoder = build_autoencoder(X_features_scaled.shape[1])\n",
    "    autoencoder.fit(X_features_scaled, X_features_scaled, epochs=50, batch_size=32, shuffle=True, verbose=0)\n",
    "\n",
    "    # Encode the features using the trained Autoencoder\n",
    "    X_encoded_features = encoder.predict(X_features_scaled)\n",
    "\n",
    "    # Train Isolation Forest for anomaly detection on the encoded features\n",
    "    iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "    anomaly_labels = iso_forest.fit_predict(X_encoded_features)\n",
    "\n",
    "    # Update labels based on anomaly detection (anomalies are labeled as 1, normal as 0)\n",
    "    updated_labels = (anomaly_labels == -1).astype(int)\n",
    "\n",
    "    # Create a DataFrame to map original samples to their generated subsamples and labels\n",
    "    original_sample_data = []\n",
    "    for idx, (original_index, window, label) in enumerate(zip(original_indices, X_sliding_windows, updated_labels)):\n",
    "        original_sample_data.append({\n",
    "            'Original Sample Index': original_index,\n",
    "            'Original Sample Label': y[original_index],\n",
    "            'Subsample Index': idx,\n",
    "            'Subsample Label': label,\n",
    "            'Subsample Data': window\n",
    "        })\n",
    "\n",
    "    df_original_samples = pd.DataFrame(original_sample_data)\n",
    "\n",
    "    # Save the DataFrame to an Excel file\n",
    "    df_original_samples.to_excel(f'{save_directory}/Original_Samples_and_Subsamples_{iteration}.xlsx', index=False)\n",
    "\n",
    "    # Split the data into training and testing sets (80% training, 20% testing)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_sliding_windows, updated_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Standardize the features (mean=0, std=1)\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Oversample the minority class using RandomOverSampler on training data\n",
    "    oversampler = RandomOverSampler(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Train a Random Forest model with early stopping\n",
    "    best_model = None\n",
    "    best_score = 0\n",
    "    no_improvement_epochs = 0\n",
    "    patience = 2\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "        train_accuracy = accuracy_score(y_train_resampled, model.predict(X_train_resampled))\n",
    "\n",
    "        if train_accuracy > best_score:\n",
    "            best_model = model\n",
    "            best_score = train_accuracy\n",
    "            no_improvement_epochs = 0\n",
    "        else:\n",
    "            no_improvement_epochs += 1\n",
    "\n",
    "        if no_improvement_epochs >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Use the best model for predictions\n",
    "    test_probabilities = best_model.predict_proba(X_test)\n",
    "    initial_predictions = best_model.predict(X_test)\n",
    "\n",
    "    # Initialize lists to store confusion matrix elements\n",
    "    tp_list = []\n",
    "    tn_list = []\n",
    "    fp_list = []\n",
    "    fn_list = []\n",
    "\n",
    "    # Initialize a table to store results for each lambda\n",
    "    table_data = []\n",
    "    abstain_table_data = []\n",
    "    metrics_table_data = []\n",
    "\n",
    "    # Initialize dictionaries to store metrics for each lambda\n",
    "    metrics_dict = {l: {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'specificity': []} for l in np.arange(0.5, 0.95, 0.05)}\n",
    "\n",
    "    # Define the range of lambda values (excluding 0.95)\n",
    "    lambdas = np.arange(0.5, 0.95, 0.05)\n",
    "\n",
    "    # Loop through lambda values and calculate metrics\n",
    "    for reject_threshold in lambdas:\n",
    "        predictions, abstain_indices = classify_with_reject(test_probabilities, reject_threshold, initial_predictions, y_test)\n",
    "\n",
    "        filtered_indices = [i for i in range(len(predictions)) if predictions[i] != -1]\n",
    "        y_test_filtered = y_test[filtered_indices]\n",
    "        predictions_filtered = predictions[filtered_indices]\n",
    "\n",
    "        if len(predictions_filtered) > 0:\n",
    "            cm = confusion_matrix(y_test_filtered, predictions_filtered, labels=[0, 1])\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "        else:\n",
    "            cm = np.array([[0, 0], [0, 0]])\n",
    "            tn, fp, fn, tp = 0, 0, 0, 0\n",
    "\n",
    "        tp_list.append(tp)\n",
    "        tn_list.append(tn)\n",
    "        fp_list.append(fp)\n",
    "        fn_list.append(fn)\n",
    "\n",
    "        table_data.append([round(reject_threshold, 2), tn, fp, fn, tp])\n",
    "\n",
    "        abstain_instances_info = []\n",
    "        for idx in abstain_indices:\n",
    "            abstain_instances_info.append((idx, y_test[idx]))\n",
    "\n",
    "        abstain_table_data.append([round(reject_threshold, 2), abstain_instances_info])\n",
    "\n",
    "        if len(y_test_filtered) > 0:\n",
    "            accuracy = accuracy_score(y_test_filtered, predictions_filtered) * 100\n",
    "            precision = precision_score(y_test_filtered, predictions_filtered, zero_division=0) * 100\n",
    "            recall = recall_score(y_test_filtered, predictions_filtered, zero_division=0) * 100\n",
    "            f1 = f1_score(y_test_filtered, predictions_filtered, zero_division=0) * 100\n",
    "            specificity = (tn / (tn + fp)) * 100 if (tn + fp) > 0 else 0\n",
    "        else:\n",
    "            accuracy = precision = recall = f1 = specificity = 0\n",
    "\n",
    "        metrics_dict[reject_threshold]['accuracy'].append(accuracy)\n",
    "        metrics_dict[reject_threshold]['precision'].append(precision)\n",
    "        metrics_dict[reject_threshold]['recall'].append(recall)\n",
    "        metrics_dict[reject_threshold]['f1'].append(f1)\n",
    "        metrics_dict[reject_threshold]['specificity'].append(specificity)\n",
    "\n",
    "        metrics_table_data.append([round(reject_threshold, 2), f\"{accuracy:.2f}%\", f\"{precision:.2f}%\", f\"{recall:.2f}%\", f\"{f1:.2f}%\", f\"{specificity:.2f}%\"])\n",
    "\n",
    "        # Show confusion matrix for each lambda\n",
    "        if cm.shape != (2, 2):\n",
    "            cm_padded = np.zeros((2, 2), dtype=int)\n",
    "            cm_padded[:cm.shape[0], :cm.shape[1]] = cm\n",
    "        else:\n",
    "            cm_padded = cm\n",
    "\n",
    "        x_labels = ['Normal', 'Abnormal']\n",
    "        y_labels = ['Abnormal', 'Normal']\n",
    "        cm_reversed = cm_padded[::-1]\n",
    "        fig = ff.create_annotated_heatmap(z=cm_reversed, x=x_labels, y=y_labels, colorscale='Blues')\n",
    "        fig.update_layout(\n",
    "            title=f'Confusion Matrix, Lambda {reject_threshold:.2f}',\n",
    "            xaxis=dict(title='Predicted labels', tickfont=dict(size=10)),\n",
    "            yaxis=dict(title='True labels', tickfont=dict(size=10)),\n",
    "            width=400,\n",
    "            height=300,\n",
    "            margin=dict(l=50, r=50, t=130, b=50)\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "        # Save figures in PNG and PDF formats\n",
    "        fig.write_image(f'{save_directory}/Confusion_Matrix_Lambda_{reject_threshold:.2f}_Iteration_{iteration}.png')\n",
    "        fig.write_image(f'{save_directory}/Confusion_Matrix_Lambda_{reject_threshold:.2f}_Iteration_{iteration}.pdf')\n",
    "\n",
    "        # Check if all metrics meet the stop criteria for this lambda\n",
    "        if accuracy >= 99 and precision >= 99 and recall >= 99 and f1 >= 99 and specificity >= 99:\n",
    "            stop_criteria = True\n",
    "            print(f\"Stopping criteria met with lambda {reject_threshold:.2f}, window size {window_size}, and step size {step_size}.\")\n",
    "            break\n",
    "\n",
    "    if not stop_criteria:\n",
    "        # Collect and average metric values\n",
    "        avg_accuracy = np.mean([np.mean(metrics_dict[l]['accuracy']) for l in lambdas])\n",
    "        \n",
    "        # Store the averaged metric value for the current window size\n",
    "        metric_values[window_size] = avg_accuracy\n",
    "        \n",
    "        # Use the golden search method to adjust the window size\n",
    "        window_size = golden_search_for_optimal_window(window_size, metric_values)\n",
    "        step_size = window_size // 2\n",
    "        print(f\"Updated window size to {window_size} and step size to {step_size}.\")\n",
    "\n",
    "    # Adjust lambdas to match the length of performance lists\n",
    "    lambdas_processed = lambdas[:len(tp_list)]\n",
    "\n",
    "    # Save average performance metrics per lambda for each iteration\n",
    "    avg_metrics_data = []\n",
    "    for l in lambdas_processed:\n",
    "        avg_accuracy = np.mean(metrics_dict[l]['accuracy'])\n",
    "        avg_precision = np.mean(metrics_dict[l]['precision'])\n",
    "        avg_recall = np.mean(metrics_dict[l]['recall'])\n",
    "        avg_f1 = np.mean(metrics_dict[l]['f1'])\n",
    "        avg_specificity = np.mean(metrics_dict[l]['specificity'])\n",
    "\n",
    "        avg_metrics_data.append([round(l, 2), f\"{avg_accuracy:.2f}%\", f\"{avg_precision:.2f}%\", f\"{avg_recall:.2f}%\", f\"{avg_f1:.2f}%\", f\"{avg_specificity:.2f}%\"])\n",
    "\n",
    "    df_avg_metrics = pd.DataFrame(avg_metrics_data, columns=['Lambda', 'Average Accuracy', 'Average Precision', 'Average Recall', 'Average F1-score', 'Average Specificity'])\n",
    "\n",
    "    # Save the table and plot as .xlsx, .png, and .pdf\n",
    "    avg_metrics_xlsx_path = f'{save_directory}/Average_Metrics_Per_Lambda_Iteration_{iteration}.xlsx'\n",
    "    df_avg_metrics.to_excel(avg_metrics_xlsx_path, index=False)\n",
    "\n",
    "    fig_avg_metrics = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df_avg_metrics.columns), fill_color='paleturquoise', align='left'),\n",
    "        cells=dict(values=[df_avg_metrics[col].tolist() for col in df_avg_metrics.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    "    )])\n",
    "    fig_avg_metrics.update_layout(width=800, height=500)\n",
    "    fig_avg_metrics.show()\n",
    "\n",
    "    # Save the figure as .png and .pdf\n",
    "    fig_avg_metrics.write_image(f'{save_directory}/Average_Performance_Metrics_vs_Lambda_Iteration_{iteration}.png')\n",
    "    fig_avg_metrics.write_image(f'{save_directory}/Average_Performance_Metrics_vs_Lambda_Iteration_{iteration}.pdf')\n",
    "\n",
    "    # Plot performance metrics for each iteration with the updated font settings\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(df_avg_metrics['Lambda'], df_avg_metrics['Average Accuracy'].str.rstrip('%').astype(float), marker='o', linestyle='-', label='Average Accuracy')\n",
    "    plt.plot(df_avg_metrics['Lambda'], df_avg_metrics['Average Precision'].str.rstrip('%').astype(float), marker='o', linestyle='-', label='Average Precision')\n",
    "    plt.plot(df_avg_metrics['Lambda'], df_avg_metrics['Average Recall'].str.rstrip('%').astype(float), marker='o', linestyle='-', label='Average Recall')\n",
    "    plt.plot(df_avg_metrics['Lambda'], df_avg_metrics['Average F1-score'].str.rstrip('%').astype(float), marker='o', linestyle='-', label='Average F1-score')\n",
    "    plt.plot(df_avg_metrics['Lambda'], df_avg_metrics['Average Specificity'].str.rstrip('%').astype(float), marker='o', linestyle='-', label='Average Specificity')\n",
    "    \n",
    "    # Update axis font to Arial with size 12\n",
    "    plt.xlabel('Lambda (Abstain Threshold)', fontsize=12, fontname='Arial')\n",
    "    plt.ylabel('Percentage', fontsize=12, fontname='Arial')\n",
    "    plt.title(f'Average Performance Metrics vs. Lambda Threshold (Iteration {iteration})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'{save_directory}/Average_Performance_Metrics_vs_Lambda_Iteration_{iteration}.png')\n",
    "    plt.savefig(f'{save_directory}/Average_Performance_Metrics_vs_Lambda_Iteration_{iteration}.pdf')\n",
    "    plt.show()\n",
    "\n",
    "    # Increment iteration counter\n",
    "    iteration += 1\n",
    "\n",
    "    print(f\"\\nAverage metrics for each lambda have been saved for iteration {iteration}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b481b3-07cb-4388-abd5-a1cf31a01f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59f289af-cfc2-4f5b-af4f-067af6205871",
   "metadata": {},
   "source": [
    "### Golden Search Algorithm Explanation\n",
    "\n",
    "The **Golden Search** algorithm operates within the code as follows:\n",
    "\n",
    "1. **Initialization**:\n",
    "   - The algorithm starts with an interval defined by `lower_bound` and `upper_bound`, which are set to half and twice the `current_window_size`, respectively.\n",
    "   - The **golden ratio** $(\\approx 1.618)$ is calculated as a constant to be used in the interval reductions.\n",
    "\n",
    "2. **Calculate Interior Points**:\n",
    "   - Two points, $x_1$ and $x_2$, are calculated within the interval $[ \\text{lower\\_bound}, \\text{upper\\_bound} ]$:\n",
    "     - $x_1$ is closer to $\\text{upper\\_bound}$, calculated as:\n",
    "       $$\n",
    "       x_1 = \\text{upper\\_bound} - \\frac{\\text{upper\\_bound} - \\text{lower\\_bound}}{\\text{golden ratio}}\n",
    "       $$\n",
    "     - $x_2$ is closer to $\\text{lower\\_bound}$, calculated as:\n",
    "       $$\n",
    "       x_2 = \\text{lower\\_bound} + \\frac{\\text{upper\\_bound} - \\text{lower\\_bound}}{\\text{golden ratio}}\n",
    "       $$\n",
    "   - These points create a smaller subinterval that maintains the golden ratio proportion, facilitating an efficient search.\n",
    "\n",
    "3. **Evaluate the Objective Function**:\n",
    "   - The algorithm retrieves the metric values at $x_1$ and $x_2$ from $\\text{metric\\_values}$ (e.g., accuracy) as $f(x_1)$ and $f(x_2)$, respectively.\n",
    "   - These evaluations are used to determine which subinterval likely contains the optimal point.\n",
    "\n",
    "4. **Compare $f(x_1)$ and $f(x_2)$**:\n",
    "   - If $f(x_1) > f(x_2)$, it suggests that the optimal value lies in the interval $[ \\text{lower\\_bound}, x_2 ]$.\n",
    "     - **Update**: Set $\\text{upper\\_bound} = x_2$, effectively discarding the interval $[ x_2, \\text{upper\\_bound} ]$.\n",
    "     - Shift $x_2$ to $x_1$ and calculate a new $x_1$ based on the updated $\\text{upper\\_bound}$.\n",
    "   - If $f(x_2) \\geq f(x_1)$, it suggests that the optimal value lies in the interval $[ x_1, \\text{upper\\_bound} ]$.\n",
    "     - **Update**: Set $\\text{lower\\_bound} = x_1$, discarding the interval $[ \\text{lower\\_bound}, x_1 ]$.\n",
    "     - Shift $x_1$ to $x_2$ and calculate a new $x_2$ based on the updated $\\text{lower\\_bound}$.\n",
    "\n",
    "5. **Check Stopping Condition**:\n",
    "   - After updating the interval, the algorithm checks if either $f(x_1)$ or $f(x_2)$ is within the desired tolerance from $\\text{target\\_metric}$.\n",
    "     - If so, the algorithm stops, and the corresponding point $x_1$ or $x_2$ is chosen as the optimal window size.\n",
    "   - If not, the search continues, narrowing the interval iteratively based on the golden ratio.\n",
    "\n",
    "6. **Termination**:\n",
    "   - The algorithm stops either when the tolerance condition is met or when it reaches the maximum number of iterations.\n",
    "   - The midpoint of the final interval, $\\frac{x_1 + x_2}{2}$, is then returned as the approximate optimal window size.\n",
    "\n",
    "The Golden Search efficiently converges to the optimal window size by consistently reducing the search interval based on the golden ratio, thereby maintaining the search proportion and speeding up the optimization process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf618ef-de5c-4bab-bcbb-b0eb51db6eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
